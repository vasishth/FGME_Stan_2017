% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@Article{Colquhoun2014,
  Title                    = {An investigation of the false discovery rate and the misinterpretation of p-values},
  Author                   = {Colquhoun, David},
  Year                     = {2014},
  Doi                      = {10.1098/rsos.140216},
  Number                   = {3},
  Pages                    = {140216},
  Volume                   = {1},

  Journal                  = {Royal Society Open Science},
  Publisher                = {The Royal Society}
}

@Book{GelmanEtAl2014,
  Title                    = {{Bayesian Data Analysis}},
  Author                   = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  Year                     = {2014},
  Edition                  = {Third Edition},
  Publisher                = {Taylor \& Francis}
}

@Article{HallerKrauss2002,
  Title                    = {Misinterpretations of significance: A problem students share with their teachers},
  Author                   = {Haller, Heiko and Krauss, Stefan},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {1--20},
  Volume                   = {7},

  Journal                  = {Methods of Psychological Research}
}

@Article{Hoekstra2014,
  Title                    = {Robust misinterpretation of confidence intervals},
  Author                   = {Hoekstra, Rink and Morey, Richard D and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  Year                     = {2014},
  Doi                      = {10.3758/s13423-013-0572-3},
  Number                   = {5},
  Pages                    = {1157--1164},
  Volume                   = {21},

  Journal                  = {Psychonomic Bulletin \& Review},
  Publisher                = {Springer}
}

@Article{HoekstraEtAl2014,
  Title                    = {Robust misinterpretation of confidence intervals},
  Author                   = {Hoekstra, Rink and Morey, Richard D and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  Year                     = {2014},
  Doi                      = {10.3758/s13423-013-0572-3},
  Number                   = {5},
  Pages                    = {1157--1164},
  Volume                   = {21},

  Journal                  = {Psychonomic Bulletin \& Review},
  Publisher                = {Springer}
}

@Book{mcelreath2015statistical,
  Title                    = {Statistical rethinking: A Bayesian course with R examples},
  Author                   = {McElreath, Richard},
  Year                     = {2015},
  ISBN                     = {9781482253443},
  Publisher                = {Chapman and Hall/CRC}
}

@Article{MoreyEtAl2015,
  Title                    = {The Fallacy of Placing Confidence in Confidence Intervals},
  Author                   = {Morey, Richard D and Hoekstra, Rink and Rouder, Jeffrey N. and Lee, Michael D and Wagenmakers, Eric-Jan},
  Year                     = {2016},
  Doi                      = {10.3758/s13423-015-0947-8},
  ISSN                     = {1531-5320},
  Number                   = {1},
  Pages                    = {103--123},
  Url                      = {http://dx.doi.org/10.3758/s13423-015-0947-8},
  Volume                   = {23},

  Abstract                 = {Interval estimates -- estimates of parameters that include an allowance for sampling uncertainty -- have long been touted as a key component of statistical analyses. There are several kinds of interval estimates, but the most popular are confidence intervals (CIs): intervals that contain the true parameter value in some known proportion of repeated samples, on average. The width of confidence intervals is thought to index the precision of an estimate; CIs are thought to be a guide to which parameter values are plausible or reasonable; and the confidence coefficient of the interval (e.g., 95 {\%}) is thought to index the plausibility that the true parameter is included in the interval. We show in a number of examples that CIs do not necessarily have any of these properties, and can lead to unjustified or arbitrary inferences. For this reason, we caution against relying upon confidence interval theory to justify interval estimates, and suggest that other theories of interval estimation should be used instead.}
}

@Article{ShiffrinEtAl2008,
  Title                    = {A Survey of Model Evaluation Approaches With a Tutorial on Hierarchical {Bayes}ian Methods},
  Author                   = {Shiffrin, Richard M. and Lee, Michael and Kim, Woojae and Wagenmakers, Eric-Jan},
  Year                     = {2008},
  Doi                      = {10.1080/03640210802414826},
  ISSN                     = {0364-0213},
  Month                    = {Dec},
  Number                   = {8},
  Pages                    = {1248--1284},
  Url                      = {http://dx.doi.org/10.1080/03640210802414826},
  Volume                   = {32},

  File                     = {:Shiffrin_et_al-2008-Cognitive_Science.pdf:PDF},
  Journal                  = {Cognitive Science},
  Publisher                = {Wiley-Blackwell}
}


@book{Lynch2007,
  title={Introduction to applied Bayesian statistics and estimation for social scientists},
  author={Lynch, Scott M.},
  year={2007},
  publisher={Springer Science \& Business Media}
}


@article{CarreirasClifton1999Anotherwordparsing,
  title = {Another Word on Parsing Relative Clauses: {{Eyetracking}} Evidence from {{Spanish}} and {{English}}},
  volume = {27},
  shorttitle = {Another Word on Parsing Relative Clauses},
  timestamp = {2016-11-27T17:11:07Z},
  number = {5},
  urldate = {2016-11-27},
  journal = {Memory \& cognition},
  author = {Carreiras, Manuel and Clifton, Charles},
  year = {1999},
  pages = {826--833},
  file = {CarreirasCliftonEyetracking.pdf:/home/bruno/.zotero/zotero/b5aal9uj.default/zotero/storage/UUJD3PNC/CarreirasCliftonEyetracking.pdf:application/pdf}
}




@article{SwetsEtAl2008Underspecificationsyntacticambiguities,
  title = {Underspecification of Syntactic Ambiguities: {{Evidence}} from Self-Paced Reading},
  volume = {36},
  issn = {0090-502X, 1532-5946},
  shorttitle = {Underspecification of Syntactic Ambiguities},
  doi = {10.3758/MC.36.1.201},
  language = {en},
  timestamp = {2016-11-29T14:41:11Z},
  number = {1},
  urldate = {2016-11-29},
  journal = {Memory \& Cognition},
  author = {Swets, Benjamin and Desmet, Timothy and Clifton, Charles and Ferreira, Fernanda},
  month = jan,
  year = {2008},
  pages = {201--216},
  file = {Swets et al 2008.pdf:/home/bruno/.zotero/zotero/b5aal9uj.default/zotero/storage/T4RIZXE2/Swets et al 2008.pdf:application/pdf}
}



@article{FrazierRayner1982making,
  title={Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences},
  author={Frazier, Lyn and Rayner, Keith},
  journal={Cognitive psychology},
  volume={14},
  number={2},
  pages={178--210},
  year={1982},
  publisher={Elsevier}
}


@book{GelmanHill2007,
  title={Data analysis using regression and multilevel/hierarchical models},
  author={Gelman, Andrew and Hill, Jennifer},
  year={2007},
  publisher={Cambridge University Press}
}


@article{Levy2008Expectationbasedsyntacticcomprehension,
  title = {Expectation-Based Syntactic Comprehension},
  volume = {106},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159-166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  timestamp = {2016-11-29T09:31:17Z},
  number = {3},
  journal = {Cognition},
  author = {Levy, Roger P.},
  month = mar,
  year = {2008},
  keywords = {Attitude,Cognition,expectations,Humans,linguistics,Models,Phd1,predictions,Psychological,Psychological Theory,Semantics},
  pages = {1126--1177},
  file = {Attachment:/home/bruno/.zotero/zotero/b5aal9uj.default/zotero/storage/668EQZIB/Levy - 2008 - Expectation-based syntactic comprehension.pdf:application/pdf},
  citation-verif = {pages; year; volume; month; number; issn; journaltitle; url; title; doi; publisher; author; were verified from given doi},
  mendeley-tags = {Phd1,expectations,predictions},
  pmid = {17662975}
}



@article{SorensenEtAl2016,
    author =    {Sorensen, Tanner AND Hohenstein, Sven AND Vasishth, Shravan },
    journal =   {The Quantitative Methods for Psychology},
    publisher = {TQMP},
    title =     {Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists},
    year =      {2016},
    volume =    {12},
    number =    {3},
    url =       {http://www.tqmp.org/RegularArticles/vol12-3/p175/p175.pdf },
    pages =     {175-200},
    abstract =  {With the arrival of the R packages \fontencoding {T1}\texttt {nlme} and \fontencoding {T1}\texttt {lme4}, linear mixed models (LMMs) have come to be widely used in experimentally-driven areas like psychology, linguistics, and cognitive science. This tutorial provides a practical introduction to fitting LMMs in a Bayesian framework using the probabilistic programming language Stan. We choose Stan (rather than WinBUGS or JAGS) because it provides an elegant and scalable framework for fitting models in most of the standard applications of LMMs. We ease the reader into fitting increasingly complex LMMs, using a two-condition repeated measures self-paced reading study.},
    doi =       {10.20982/tqmp.12.3.p175}
}

@Book{WagenmakersLee2013book,
  Title                    = {Bayesian cognitive modeling: A practical course},
  Author                   = {Lee, Michael D and Wagenmakers, Eric-Jan},
  Year                     = {2013},
  Publisher                = {Cambridge University Press}
}

@Misc{Stan2017,
  Title                    = {Stan: {A} {C++} Library for Probability and Sampling, Version
 2.16.0},
  Author                   = {{Stan Development Team}},
  Year                     = {2017},
  Url                      = {http://mc-stan.org/}
}

@Article{Ratcliff1998,
  Title                    = {Modeling Response Times for Two-Choice Decisions},
  Author                   = {Ratcliff, Roger and Rouder, Jeffrey N.},
  Journaltitle             = {Psychological Science},
  Year                     = {1998},
  Doi                      = {10.1111/1467-9280.00067},
  ISSN                     = {0956-7976},
  Month                    = {Sep},
  Number                   = {5},
  Pages                    = {347--356},
  Url                      = {http://dx.doi.org/10.1111/1467-9280.00067},
  Volume                   = {9},

  Citation-verif           = {pages; year; volume; month; number; issn; journaltitle; url; title; doi; publisher; author; were verified from given doi},
  File                     = {:Ratcliff, Rouder - 1998 - Modeling Response Times for Two-Choice Decisions.pdf:PDF},
  Keywords                 = {Pavel,Phd1,modeling},
  Mendeley-tags            = {Pavel,Phd1,modeling},
  Publisher                = {Sage Publications}
}
@article{RatcliffEtAl2016,
title = "Diffusion Decision Model: {C}urrent Issues and History ",
journal = "Trends in Cognitive Sciences ",
volume = "20",
number = "4",
pages = "260 -- 281",
year = "2016",
note = "",
issn = "1364-6613",
doi = "10.1016/j.tics.2016.01.007",
url = "http://www.sciencedirect.com/science/article/pii/S1364661316000255",
author = "Roger Ratcliff and Philip L. Smith and Scott D. Brown and Gail McKoon",
keywords = "diffusion model",
keywords = "response time",
keywords = "optimality",
keywords = "nonstationarity "
}

@article{Ratcliff1978,
  title={A theory of memory retrieval.},
  author={Ratcliff, Roger},
  journal={Psychological review},
  volume={85},
  number={2},
  pages={59},
  year={1978},
  publisher={American Psychological Association}
}


@Article{RouderEtAl2014,
  Title                    = {The Lognormal Race: {A} Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties},
  Author                   = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
  Year                     = {2014},
  Doi                      = {10.1007/s11336-013-9396-3},
  ISSN                     = {1860-0980},
  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {491–513},
  Url                      = {http://dx.doi.org/10.1007/S11336-013-9396-3},
  Volume                   = {80},

  File                     = {:home/bruno/Downloads/rouder2015.pdf:PDF},
  Journal                  = {Psychometrika},
  Publisher                = {Springer Science + Business Media}
}

@Article{HeathcoteLove2012,
  Title                    = {Linear Deterministic Accumulator Models of Simple Choice},
  Author                   = {Heathcote, Andrew and Love, Jonathon},
  Year                     = {2012},
  Doi                      = {10.3389/fpsyg.2012.00292},
  ISSN                     = {1664-1078},
  Url                      = {http://dx.doi.org/10.3389/fpsyg.2012.00292},
  Volume                   = {3},

  File                     = {:home/bruno/ownCloud/Papers/new-rts more/diff/Heathcote Love 2012.pdf:PDF},
  Journal                  = {Frontiers in Psychology},
  Publisher                = {Frontiers Media SA}
}

@Article{Rouder2005,
  Title                    = {Are unshifted distributional models appropriate for response time?},
  Author                   = {Rouder, Jeffrey N.},
  Year                     = {2005},
  Doi                      = {10.1007/s11336-005-1297-7},
  ISSN                     = {1860-0980},
  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {377–381},
  Url                      = {http://dx.doi.org/10.1007/s11336-005-1297-7},
  Volume                   = {70},

  File                     = {:Rouder-2005 shifted lognormal.pdf:PDF},
  Journal                  = {Psychometrika},
  Publisher                = {Springer Science + Business Media}
}

@article{Gelman2013,
  title={Two simple examples for understanding posterior p-values whose distributions are far from unform},
  author={Gelman, Andrew and others},
  journal={Electronic Journal of Statistics},
  volume={7},
  pages={2595--2602},
  year={2013},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@book{kruschke2015doing,
  title                = {Doing {B}ayesian Data Analysis},
  author                   = {Kruschke, John K. },
  Year                     = {2015},
  Doi                      = {http://dx.doi.org/10.1016/B978-0-12-405888-0.09999-2},
  Edition                  = {Second},
  ISBN                     = {978-0-12-405888-0},
  Publisher                = {Academic Press},
  Address                  = {Boston}
  }



@article {MonnahanEtAl2017,
author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
title = {Faster estimation of Bayesian models in ecology using Hamiltonian Monte Carlo},
journal = {Methods in Ecology and Evolution},
volume = {8},
number = {3},
issn = {2041-210X},
url = {http://dx.doi.org/10.1111/2041-210X.12681},
doi = {10.1111/2041-210X.12681},
pages = {339--348},
keywords = {Bayesian inference, hierarchical modelling, Markov chain Monte Carlo, no-U-turn sampler, Stan},
year = {2017},
}

@article{lunn2000winbugs,
  title={{WinBUGS}-{A B}ayesian modelling framework: {C}oncepts, structure, and extensibility},
  author={Lunn, D.J. and Thomas, A. and Best, N. and Spiegelhalter, D.},
  journal={Statistics and computing},
  volume={10},
  number={4},
  pages={325--337},
  year={2000},
  publisher={Springer}
}


@misc{plummer2016jags,
  Author = {Plummer, Martin},
  Title = {JAGS Version 4.2.0 user manual},
  Year = {2016}}


@article{VasishthetalPLoSOne2013,
    author = {Vasishth, Shravan and Chen, Zhong and Li, Qiang and Guo, Gueilan},
    journal = {PLoS ONE},
    publisher = {Public Library of Science},
    title = {Processing {C}hinese Relative Clauses: {E}vidence for the Subject-Relative Advantage},
    year = {2013},
    month = {10},
    volume = {8},
    pdf = {http://dx.doi.org/10.1371%2Fjournal.pone.0077006},
    pages = {1--14},
    abstract = {A general fact about language is that subject relative clauses are easier to process than object relative clauses. Recently, several self-paced reading studies have presented surprising evidence that object relatives in Chinese are easier to process than subject relatives. We carried out three self-paced reading experiments that attempted to replicate these results. Two of our three studies found a subject-relative preference, and the third study found an object-relative advantage. Using a random effects Bayesian meta-analysis of fifteen studies (including our own), we show that the overall current evidence for the subject-relative advantage is quite strong (approximate posterior probability of a subject-relative advantage given the data: 78–80%). We argue that retrieval/integration based accounts would have difficulty explaining all three experimental results. These findings are important because they narrow the theoretical space by limiting the role of an important class of explanation—retrieval/integration cost—at least for relative clause processing in Chinese.},
    number = {10},
    code = {http://www.ling.uni-potsdam.de/~vasishth/code/PLoSOneVasishthetaldata.zip}
}        

@Article{JaegerEtAl2017,
  Title                    = {Similarity-based interference in sentence comprehension: {Literature} review and {Bayesian} meta-analysis
},
  Author                   = {J{\"a}ger, Lena Ann and Engelmann, Felix and  Vasishth, Shravan},
 journal = {Journal of Memory and Language},
 volume = {94},
 pages = {316--339},
 year = {2017}
}

@misc{NicenboimEtAl2016NIG,
  title={Number interference in {German}: {Evidence} for cue-based retrieval},
  url={osf.io/mmr7s},
  preprint={osf.io/mmr7s},
  DOI={10.17605/OSF.IO/MMR7S},
  publisher={Open Science Framework},
  author={Nicenboim, Bruno and Engelmann, Felix and Suckow, Katja and Vasishth, Shravan},
  year={Submitted},
  month={Dec}
}



@ARTICLE{NicenboimVasishth2016,
   author = {Bruno Nicenboim and Shravan Vasishth},
    title = "{Statistical methods for linguistic research: {Foundational} Ideas - {Part} {II}}",
  journal = {Language and Linguistics Compass},
   eprint = {https://arxiv.org/abs/1602.00245},
  pages = {591--613},
  doi = {10.1111/lnc3.12207},
url = {http://dx.doi.org/10.1111/lnc3.12207},
  issn = {1749-818X},
  number = {11},
  volume = {10},
     year = "2016"
}

@Article{NicenboimEtAl2016Frontiersb,
  Title                    = { When high-capacity readers slow down and low-capacity readers speed up: {W}orking memory and locality effects },
  Author                   = { Bruno Nicenboim and Pavel Logačev and Carolina Gattei and Shravan Vasishth },
  JOURNAL={Frontiers in Psychology},      
  VOLUME={7},      
  YEAR={2016},      
  NUMBER={280},     
  URL={http://www.frontiersin.org/language_sciences/10.3389/fpsyg.2016.00280/abstract},       
  DOI={10.3389/fpsyg.2016.00280},      
  ISSN={1664-1078} ,      
  ABSTRACT={We examined the effects of argument-head distance in SVO and SOV languages (Spanish and German), while taking into account readers’ working memory capacity and controlling for expectation (Levy, 2008) and other factors. We predicted only locality effects, that is, a slow-down produced by increased dependency distance (Gibson, 2000; Lewis & Vasishth, 2005). Furthermore, we expected stronger locality effects for readers with low working memory capacity. Contrary to our predictions, low-capacity readers showed faster reading with increased distance, while high-capacity readers showed locality effects. We suggest that while the locality effects are compatible with memory-based explanations, the speedup of low-capacity readers can be explained by an increased probability of retrieval failure. We present a computational model based on ACT-R built under the previous assumptions, which is able to give a qualitative account for the present data and can be tested in future research. Our results suggest that in some cases, interpreting longer RTs as indexing increased processing difficulty and shorter RTs as facilitation may be too simplistic: The same increase in processing difficulty may lead to slowdowns in high-capacity readers and speedups in low-capacity ones. Ignoring individual level capacity differences when investigating locality effects may lead to misleading conclusions.}
}


@unpublished{NicenboimVasishth2016Models,
  Title                    = {Models of Retrieval in Sentence Comprehension: {A} computational evaluation using {Bayesian} hierarchical modeling},
  Author                   = { Bruno Nicenboim and Shravan Vasishth },
  Year                     = {Submitted},
  eprint = {https://arxiv.org/abs/1612.04174},
  notes={Under review in Journal of Memory and Language; arxiv e-print},
  url                      = {https://arxiv.org/abs/1612.04174},
  customa = {papers/NicenboimVasishth2016Models.pdf}
}


@Inproceedings{VasishthEtAl2017Modelling,
  Title                    = {Modelling dependency completion in sentence comprehension as a {Bayesian} hierarchical mixture process: {A} case study involving {Chinese} relative clauses},
  Author                   = {Shravan Vasishth and Nicolas Chopin and Robin Ryder and Bruno Nicenboim},
  eprint = {https://arxiv.org/abs/1702.00564v2},
       year = 2017,
  notes={arxiv e-print},
  Booktitle={Proceedings of Cognitive Science Conference},
Location={London, UK},
  url                      = {https://arxiv.org/abs/1702.00564v2},
  customa = {papers/VasishthEtAl2017Modelling.pdf}
}


@Inproceedings{VasishthEtAl2017Feature,
   author = {{Vasishth}, S. and {J{\"a}ger}, L.~A. and {Nicenboim}, B.},
    title = "{Feature overwriting as a finite mixture process: {Evidence} from comprehension data}",
archivePrefix = "arXiv",
   eprint = {https://arxiv.org/abs/1703.04081},
     Booktitle={Proceedings of MathPsych/ICCM Conference},
Location={Warwick, UK},
 notes={arxiv e-print},
     year = 2017,
  url = {https://arxiv.org/abs/1703.04081},
  customa = {papers/VasishthEtAl2017Feature.pdf}
  
}

@article{HofmeisterVasishth2014,
  author = {Philip Hofmeister and Shravan Vasishth},
  title = {Distinctiveness and encoding effects in online sentence comprehension},
  year = {2014},
  pages = {1--13},
  abstract = {In explicit memory recall and recognition tasks, elaboration and contextual isolation both facilitate memory performance. Here, we investigate these effects in the context of sentence processing: targets for retrieval during online sentence processing of English object relative clause constructions differ in the amount of elaboration associated with the target noun phrase, or the homogeneity of superficial features (text color). Experiment 1 shows that greater elaboration for targets during the encoding phase reduces reading times at retrieval sites, but elaboration of non-targets has considerably weaker effects. Experiment 2 illustrates that processing isolated superficial features of target noun phrases—here, a green word in a sentence with words colored white—does not lead to enhanced memory performance, despite triggering longer encoding times. These results are interpreted in the light of the memory models of Nairne, 1990, 2001, 2006, which state that encoding remnants contribute to the set of retrieval cues that provide the basis for similarity-based interference effects.},
  doi = {doi: 10.3389/fpsyg.2014.01237},
  pdf = {http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.01237/abstract},
  volume = {5},
  journal = {Frontiers in Psychology},
  note = {Article 1237},
  code = {http://privatewww.essex.ac.uk/~phofme/Univ3-Frontiers.zip}
}


@article{HusainEtAl2014,
  title={Strong Expectations Cancel Locality Effects: {E}vidence from {H}indi},
  author={Husain, Samar and Vasishth, Shravan and Srinivasan, Narayanan},
  journal={PLoS ONE},
  volume={9},
  number={7},
  pages={1--14},
  year={2014},
  abstract = {Expectation-driven facilitation (Hale, 2001; Levy, 2008) and locality-driven retrieval difficulty (Gibson, 1998, 2000; Lewis &
Vasishth, 2005) are widely recognized to be two critical factors in incremental sentence processing; there is accumulating
evidence that both can influence processing difficulty. However, it is unclear whether and how expectations and memory
interact. We first confirm a key prediction of the expectation account: a Hindi self-paced reading study shows that when an
expectation for an upcoming part of speech is dashed, building a rarer structure consumes more processing time than
building a less rare structure. This is a strong validation of the expectation-based account. In a second study, we show that
when expectation is strong, i.e., when a particular verb is predicted, strong facilitation effects are seen when the appearance
of the verb is delayed; however, when expectation is weak, i.e., when only the part of speech ``verb'' is predicted but a
particular verb is not predicted, the facilitation disappears and a tendency towards a locality effect is seen. The interaction
seen between expectation strength and distance shows that strong expectations cancel locality effects, and that weak
expectations allow locality effects to emerge.},
  publisher={Public Library of Science},
  pdf = {http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0100986},
  code={http://www.ling.uni-potsdam.de/~vasishth/code/HusainEtAl2014PLoSONE.zip}
}

@article{FrankEtAl2015,
  author = {Stefan L. Frank and Thijs Trompenaars and Shravan Vasishth},
  title = {Cross-linguistic differences in processing double-embedded relative clauses: {W}orking-memory constraints or language statistics?},
  year = {2015},
  pages = {n/a},
  abstract = {An English double-embedded relative clause from which the middle verb is omitted can often be processed more easily than its grammatical counterpart, a phenomenon known as the grammaticality illusion. This effect has been found to be reversed in German, suggesting that the illusion is language specific rather than a consequence of universal working memory constraints. We present results from three self-paced reading experiments which show that Dutch native speakers also do not show the grammaticality illusion in Dutch, whereas both German and Dutch native speakers do show the illusion when reading English sentences. These findings provide evidence against working memory constraints as an explanation for the observed effect in English. We propose an alternative account based on the statistical patterns of the languages involved. In support of this alternative, a single recurrent neural network model that is trained on both Dutch and English sentences indeed predicts the cross-linguistic difference in grammaticality effect.},
  journal = {Cognitive Science},
  code = {https://github.com/vasishth/StanJAGSexamples/tree/master/FrankEtAlCogSci2015},
  pdf = {http://www.ling.uni-potsdam.de/~vasishth/pdfs/FrankTrompenaarsVasishthCogSci.pdf}
}

@Article{Lee2011,
  Title                    = {How cognitive modeling can benefit from hierarchical {Bayes}ian models},
  Author                   = {Lee, Michael D.},
  Year                     = {2011},
  Doi                      = {10.1016/j.jmp.2010.08.013},
  ISSN                     = {0022-2496},
  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {1--7},
  Url                      = {http://dx.doi.org/10.1016/j.jmp.2010.08.013},
  Volume                   = {55},

  File                     = {:Lee-inpress HIERARCHICAL.pdf:PDF},
  Journal                  = {Journal of Mathematical Psychology},
  Publisher                = {Elsevier BV}
}


@book{LeeWagenmarks2014,
  title={Bayesian cognitive modeling: A practical course},
  author={Lee, Michael D. and Wagenmakers, Eric-Jan},
  year={2014},
  publisher={Cambridge University Press}
}

@article {LogacevVasishth2015,
author = {Logačev, Pavel and Vasishth, Shravan},
title = {A Multiple-Channel Model of Task-Dependent Ambiguity Resolution in Sentence Comprehension},
journal = {Cognitive Science},
volume = {40},
number = {2},
issn = {1551-6709},
url = {http://dx.doi.org/10.1111/cogs.12228},
doi = {10.1111/cogs.12228},
pages = {266--298},
keywords = {Sentence processing, Ambiguity, Parallel processing, Cognitive modeling, Unrestricted race model, URM, Underspecification, Good-enough processing},
year = {2016},
}


@unpublished{BatesEtAlParsimonious,
  Author = {Douglas Bates and Reinhold Kliegl and Shravan Vasishth and Harald Baayen},
  Note = {ArXiv e-print},
  Title = {Parsimonious mixed models},
  Year = {2015},
  url = {http://arxiv.org/abs/1506.04967},
    abstract = {The analysis of experimental data with mixed-effects models requires
decisions about the specification of the appropriate random-effects structure.
Recently, Barr, et al 2013, recommended  fitting `maximal'
models with all possible random effect components included.  Estimation of
maximal models, however, may not converge.  We show that failure to converge
 typically is not due to a suboptimal estimation algorithm, but is
a consequence of attempting to fit a model that is too complex to be properly
supported by the data, irrespective of whether estimation is based on maximum
likelihood or on Bayesian hierarchical modeling with uninformative or weakly
informative priors.  Importantly, even under convergence, overparameterization
may lead to uninterpretable models.  We provide diagnostic tools for detecting
overparameterization and guiding model simplification.  Finally, we clarify
that the simulations on which Barr et al. base their recommendations are
atypical for real data.  A detailed example is provided of how subject-related
attentional fluctuation across trials may further qualify
statistical inferences about fixed effects, and of how such nonlinear effects
can be accommodated within the mixed-effects modeling framework.}
}


@article{GelmanEtAl2014understanding,
  title={Understanding predictive information criteria for {B}ayesian models},
  author={Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
  journal={Statistics and Computing},
  volume={24},
  number={6},
  pages={997--1016},
  doi={10.1007/s11222-013-9416-2},
  year={2014},
  publisher={Springer}
}


@Article{VehtariOjanen2012,
  Title                    = {A survey of {Bayes}ian predictive methods for model assessment, selection and comparison},
  Author                   = {Vehtari, Aki and Ojanen, Janne},
  Year                     = {2012},
  Doi                      = {10.1214/12-ss102},
  ISSN                     = {1935-7516},
  Number                   = {0},
  Pages                    = {142--228},
  Url                      = {http://dx.doi.org/10.1214/12-SS102},
  Volume                   = {6},

  File                     = {:home/bruno/ownCloud/Papers/stats/Vehtari Ojanen 2012 A survey of Bayesian predictive methods for model assessment, selection and comparison.pdf:PDF},
  Journal                  = {Statistics Surveys},
  Publisher                = {Institute of Mathematical Statistics}
}

@Article{VehtariEtAl2017,
author="Vehtari, Aki
and Gelman, Andrew
and Gabry, Jonah",
title="Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}",
journal="Statistics and Computing",
year="2017",
volume="27",
number="5",
pages="1413--1432",
abstract="Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.",
issn="1573-1375",
doi="10.1007/s11222-016-9696-4",
url="http://dx.doi.org/10.1007/s11222-016-9696-4"
}
